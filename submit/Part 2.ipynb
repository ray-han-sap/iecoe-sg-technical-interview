{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Natural Language Processing\n",
    "Codes executed in `submit` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import from required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thefo\\documents\\git_repos\\iecoe-sg-technical-interview\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, BatchNormalization, Dropout, Dense\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import pickle\n",
    "from typing import List\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download stopwords from `nltk` library (will result in error later on if this step is not done right after installing `nltk`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thefo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define directories and create temporary directory `tmp` for storing intermediate objects (e.g. trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = Path.cwd()\n",
    "DATA_PATH = CURR_DIR.parent /'fake-and-real-news-dataset.zip'\n",
    "TMP_DIR = CURR_DIR.parent / 'tmp'\n",
    "\n",
    "if not TMP_DIR.exists():\n",
    "    TMP_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data files and merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from zip file\n",
    "zip_file = ZipFile(DATA_PATH)\n",
    "fake_df, real_df = [\n",
    "    pd.read_csv(zip_file.open(text_file.filename))\n",
    "    for text_file in zip_file.infolist()\n",
    "    if text_file.filename.endswith('.csv')\n",
    "]\n",
    "\n",
    "# Give label of 1 for fake news and 0 for real news\n",
    "fake_df['label'] = 1\n",
    "real_df['label'] = 0\n",
    "\n",
    "# Combine DataFrames in a single one\n",
    "input_df = pd.concat([fake_df, real_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine top 5 rows of input `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  label  \n",
       "0  December 31, 2017      1  \n",
       "1  December 31, 2017      1  \n",
       "2  December 30, 2017      1  \n",
       "3  December 29, 2017      1  \n",
       "4  December 25, 2017      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data type for every column and also for missing values (no missing values found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    44898 non-null  object\n",
      " 1   text     44898 non-null  object\n",
      " 2   subject  44898 non-null  object\n",
      " 3   date     44898 non-null  object\n",
      " 4   label    44898 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split dataset into training and test sets, keeping proportion of positive and negative labels constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(input_df, test_size=0.25, stratify=input_df['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to perform text processing and obtain corpuses from news titles. Corpus is saved in serialized file to reduce runtime during repeated execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def process_sentence(title: str) -> str:\n",
    "    # Procedure from https://www.kaggle.com/dheerajchaudhary/simple-nlp-model-using-word2vec\n",
    "    review = re.sub('[^a-zA-Z]', ' ', title)\n",
    "    review = review.lower()\n",
    "\n",
    "    review = [\n",
    "        porter_stemmer.stem(word)\n",
    "        for word in review.split()\n",
    "        if not word in stopwords.words('english')\n",
    "    ]\n",
    "    return ' '.join(review)\n",
    "\n",
    "\n",
    "def get_corpus(corpus_path: Path, titles: List[str]) -> List[str]:\n",
    "    if corpus_path.exists():\n",
    "        with open(corpus_path, 'rb') as pkl_file:\n",
    "            corpus = pickle.load(pkl_file)\n",
    "    else:\n",
    "        corpus = [process_sentence(title) for title in titles]\n",
    "        with open(corpus_path, 'wb') as pkl_file:\n",
    "            pickle.dump(corpus, pkl_file)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get corresponding corpuses from training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus: List[str] = []\n",
    "test_corpus: List[str] = []\n",
    "TRAIN_CORPUS_PATH = TMP_DIR / 'train_corpus.pkl'\n",
    "TEST_CORPUS_PATH = TMP_DIR / 'test_corpus.pkl'\n",
    "\n",
    "train_corpus = get_corpus(TRAIN_CORPUS_PATH, train_df['title'].tolist())\n",
    "test_corpus = get_corpus(TEST_CORPUS_PATH, test_df['title'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF transform on training and test corpuses (for model 1). TF-IDF Vectorizer is saved in serialized file to reduce runtime during repeated execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X_train_tfidf: (33673, 12211)\n",
      "Dimensions of X_test_tfidf: (11225, 12211)\n"
     ]
    }
   ],
   "source": [
    "VECTORIZER_PATH = TMP_DIR / 'tfidf_vectorizer.pkl'\n",
    "\n",
    "vectorizer: TfidfVectorizer\n",
    "\n",
    "if VECTORIZER_PATH.exists():\n",
    "    vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_corpus)\n",
    "    joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "\n",
    "\n",
    "X_train_tfidf = vectorizer.transform(train_corpus).toarray()\n",
    "X_test_tfidf = vectorizer.transform(test_corpus).toarray()\n",
    "\n",
    "print(f'Dimensions of X_train_tfidf: {X_train_tfidf.shape}')\n",
    "print(f'Dimensions of X_test_tfidf: {X_test_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec transform on training and test corpuses (for model 2). Doc2Vec embeddings are saved to file after training and loaded when needed, to reduce runtime during repeated executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X_train_doc2vec: (33673, 32)\n",
      "Dimensions of X_test_doc2vec: (11225, 32)\n"
     ]
    }
   ],
   "source": [
    "# Processing steps from\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n",
    "DOC2VEC_PATH = TMP_DIR / 'doc2vec.model'\n",
    "VEC_SIZE = 32\n",
    "fname = get_tmpfile(DOC2VEC_PATH)\n",
    "\n",
    "if DOC2VEC_PATH.exists():\n",
    "    doc2vec = Doc2Vec.load(fname)\n",
    "else:\n",
    "    train_documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(train_corpus)]\n",
    "    doc2vec = Doc2Vec(vector_size=VEC_SIZE, min_count=0)\n",
    "    doc2vec.build_vocab(train_documents)\n",
    "    doc2vec.train(train_documents, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)\n",
    "    doc2vec.save(fname)\n",
    "\n",
    "X_train_doc2vec = np.array([\n",
    "    doc2vec.infer_vector(doc.split())\n",
    "    for doc in train_corpus\n",
    "])\n",
    "X_test_doc2vec = np.array([\n",
    "    doc2vec.infer_vector(doc.split())\n",
    "    for doc in test_corpus\n",
    "])\n",
    "\n",
    "print(f'Dimensions of X_train_doc2vec: {X_train_doc2vec.shape}')\n",
    "print(f'Dimensions of X_test_doc2vec: {X_test_doc2vec.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding for model 3. To be used for embedding later. One-hot encoded vectors are padded to ensure equal length for all documents in corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X_train_ohe: (33673, 20)\n",
      "Dimensions of X_test_ohe: (11225, 20)\n"
     ]
    }
   ],
   "source": [
    "# Processing steps from\n",
    "# https://www.kaggle.com/dheerajchaudhary/simple-nlp-model-using-word2vec\n",
    "VOCAB_SIZE = 10000\n",
    "SENTENCE_LEN = 20\n",
    "\n",
    "X_train_ohe = pad_sequences(\n",
    "    [one_hot(doc, VOCAB_SIZE) for doc in train_corpus],\n",
    "    padding='pre',\n",
    "    maxlen=SENTENCE_LEN\n",
    ")\n",
    "X_test_ohe = pad_sequences(\n",
    "    [one_hot(doc, VOCAB_SIZE) for doc in test_corpus],\n",
    "    padding='pre',\n",
    "    maxlen=SENTENCE_LEN\n",
    ")\n",
    "\n",
    "print(f'Dimensions of X_train_ohe: {X_train_ohe.shape}')\n",
    "print(f'Dimensions of X_test_ohe: {X_test_ohe.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get target labels from training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_train: 33673\n",
      "Lenght of y_test: 11225\n"
     ]
    }
   ],
   "source": [
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(f'Length of y_train: {len(y_train)}')\n",
    "print(f'Lenght of y_test: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes\n",
    "Gaussian distribution assumed for underlying model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Naive Bayes model. Model is saved after training to reduce runtime during repeated executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NB_PATH = TMP_DIR / 'model_nb.pkl'\n",
    "\n",
    "if MODEL_NB_PATH.exists():\n",
    "    model_nb = joblib.load(MODEL_NB_PATH)\n",
    "else:\n",
    "    model_nb = GaussianNB()\n",
    "    model_nb.fit(X_train_tfidf, y_train)\n",
    "    joblib.dump(model_nb, MODEL_NB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate inference time, accuracy and F1 score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.791\n",
      "F1 score on test set: 0.766\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_nb.predict(X_test_tfidf)\n",
    "print(f'Accuracy on test set: {accuracy_score(y_test, y_pred):.3f}')\n",
    "print(f'F1 score on test set: {f1_score(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Gradient Boosted Trees model. Model is saved after training to reduce runtime during repeated executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GBM_PATH = TMP_DIR / 'model_gbm.pkl'\n",
    "\n",
    "if MODEL_GBM_PATH.exists():\n",
    "    model_gbm = joblib.load(MODEL_GBM_PATH)\n",
    "else:\n",
    "    model_gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "    model_gbm.fit(X_train_doc2vec, y_train)\n",
    "    joblib.dump(model_gbm, MODEL_GBM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate inference time, accuracy and F1 score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.830\n",
      "F1 score on test set: 0.836\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_gbm.predict(X_test_doc2vec)\n",
    "print(f'Accuracy on test set: {accuracy_score(y_test, y_pred):.3f}')\n",
    "print(f'F1 score on test set: {f1_score(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Deep Learning\n",
    "Contains Embedding, LSTM, Batch Normalization, Dropout and Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define deep learning model architecture and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning architecture from\n",
    "# https://www.kaggle.com/dheerajchaudhary/simple-nlp-model-using-word2vec\n",
    "NUM_EMBEDDING = 60\n",
    "inp = Input(shape=(SENTENCE_LEN,), name='input')\n",
    "out = Embedding(VOCAB_SIZE, NUM_EMBEDDING, name='embedding')(inp)\n",
    "out = LSTM(64, return_sequences=True, name='lstm_0')(out)\n",
    "out = BatchNormalization(name='batchnorm_0')(out)\n",
    "out = Dropout(0.5, name='dropout_0')(out)\n",
    "out = LSTM(32, name='lstm_1')(out)\n",
    "out = BatchNormalization(name='batchnorm_1')(out)\n",
    "out = Dropout(0.5, name='dropout_1')(out)\n",
    "out = Dense(1, activation='relu', name='dense_0')(out)\n",
    "out = Dense(1, activation='sigmoid', name='dense_1')(out)\n",
    "model_dl = Model(inputs=inp, outputs=out, name='functional_model')\n",
    "model_dl.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train deep learning model using LSTM layers. Model is saved after training to reduce runtime during repeated executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DL_PATH = TMP_DIR / 'model_dl.h5'\n",
    "\n",
    "if MODEL_DL_PATH.exists():\n",
    "    model_dl = load_model(MODEL_DL_PATH)\n",
    "else:\n",
    "    # Define early stopping callback to stop training process when\n",
    "    # there is no more significant improvement in training score\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    model_dl.fit(X_train_ohe, y_train, epochs=20, batch_size=128, callbacks=[early_stopping])\n",
    "    model_dl.save(MODEL_DL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate inference time, accuracy and F1 score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.635\n",
      "F1 score on test set: 0.687\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "# Apply threshold to outputted prediction, to transform probability values\n",
    "# to binary labels\n",
    "y_pred = np.where(model_dl.predict(X_test_ohe).flatten() > THRESHOLD, 1, 0)\n",
    "print(f'Accuracy on test set: {accuracy_score(y_test, y_pred):.3f}')\n",
    "print(f'F1 score on test set: {f1_score(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons for model choice:\n",
    "- Naive Bayes: The idea was to choose a relatively simple model to start off as a baseline, and try to improve from there.\n",
    "- Gradient Boosted Trees: Gradient Boosted Trees model is one of the non deep learning models that is known for high performance in terms of metrics, so it is a model worth trying.\n",
    "- Deep Learning: Deep learning is known to perform well for unstructured datasets (e.g. images, text). LSTM layers are suited for sequences, such as words and sentences, and it has the ability to learn and understand from long sequences of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between models:\n",
    "\n",
    "Model|Accuracy|F1 Score\n",
    "---|---|---\n",
    "Naive Bayes|0.791|0.766\n",
    "Gradient Boosted Trees|0.830|0.836\n",
    "Deep Leaarning|0.635|0.687\n",
    "\n",
    "Based on the accuracy and F1 score, the Gradient Boosted Trees model is the most preferred. However, it is to be noted that other processes, such as feature engineering and hyperparamter tuning, could change these metrics, so these current metrics probably should not be treated as the endpoint of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
