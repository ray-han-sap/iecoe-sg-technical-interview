{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if working in jupyter notebook\n",
    "# %load_ext nb_black\n",
    "## if working in jupyter lab\n",
    "#%load_ext lab_black\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download Kaggle dataset\n",
    "Dataset source: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/\n",
    "- The following cells use the Kaggle API to download the dataset, please follow these [instructions](https://github.com/Kaggle/kaggle-api#api-credentials) in creating your Kaggle API Token if you have not already done so.\n",
    "- After downloading your `kaggle.json` make sure to edit the KAGGLE_JSON variable so it can find the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 42336\n",
      "drwxrwxr-x 3 evan evan     4096 Mar 25 17:25 .\n",
      "drwxrwxr-x 8 evan evan     4096 Mar 27 21:58 ..\n",
      "-rw-rw-r-- 1 evan evan   338769 Mar 20 12:40 df_cases_200906.gzip\n",
      "-rw-rw-r-- 1 evan evan    19689 Mar 20 12:40 df_label_200906.gzip\n",
      "-rw-rw-r-- 1 evan evan 42975911 Mar 25 17:17 fake-and-real-news-dataset.zip\n",
      "-rw-rw-r-- 1 evan evan        0 Mar 20 12:40 .gitkeep\n",
      "drwxrwxr-x 2 evan evan     4096 Mar 27 21:29 kaggle\n"
     ]
    }
   ],
   "source": [
    "# check that our .gzip files are present\n",
    "data_path = \"../data\"\n",
    "!ls -la $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "KAGGLE_JSON = HOME + \"/.kaggle/kaggle.json\"\n",
    "\n",
    "f = open(KAGGLE_JSON)\n",
    "data = json.load(f)\n",
    "\n",
    "USER = data.get(\"username\")\n",
    "KEY = data.get(\"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KAGGLE_CONFIG_DIR=/home/evan/.kaggle/kaggle.json\n",
      "env: KAGGLE_USERNAME=evantancy\n",
      "env: KAGGLE_KEY=8d05a9f599eddbe903759ebe9b8bb56f\n",
      "/home/evan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%env KAGGLE_CONFIG_DIR=$KAGGLE_JSON\n",
    "!chmod 600 $KAGGLE_CONFIG_DIR\n",
    "%env KAGGLE_USERNAME=$USER\n",
    "%env KAGGLE_KEY=$KEY\n",
    "!ls $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake-and-real-news-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset -p $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../data/fake-and-real-news-dataset.zip\n",
      "  inflating: ../data/kaggle/Fake.csv  \n",
      "  inflating: ../data/kaggle/True.csv  \n"
     ]
    }
   ],
   "source": [
    "!rm $data_path/kaggle/True.csv\n",
    "!rm $data_path/kaggle/Fake.csv\n",
    "!unzip $data_path/fake-and-real-news-dataset.zip -d $data_path/kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Kaggle dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /home/evan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all of our imported libraries\n",
    "import datetime\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.text as text\n",
    "import tensorflow.keras.preprocessing.sequence as sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    Dropout,\n",
    "    Conv1D,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling1D,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real and fake news DataFrames\n",
    "df_real = pd.read_csv(\"../data/kaggle/True.csv\")\n",
    "df_fake = pd.read_csv(\"../data/kaggle/Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Explore real news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any NaN values\n",
    "df_real.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politicsNews    11272\n",
       "worldnews       10145\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real[\"subject\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add an is_fake column as a label\n",
    "df_real[\"is_fake\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Explore fake news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News               9050\n",
       "politics           6841\n",
       "left-news          4459\n",
       "Government News    1570\n",
       "US_News             783\n",
       "Middle-east         778\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake[\"subject\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add an is_fake column as a label\n",
    "df_fake[\"is_fake\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both DataFrames (vertical concatenation)\n",
    "df_all_news = pd.concat([df_real, df_fake], ignore_index=True)\n",
    "# shuffle and reindex the new DataFrame\n",
    "df_all_news = df_all_news.sample(frac=1).reset_index(drop=True)\n",
    "# drop subject, date, and title column as they're different\n",
    "df_all_news = df_all_news.drop([\"subject\", \"date\", \"title\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITED NATIONS (Reuters) - The United States a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYDNEY (Reuters) - Thousands of people rallied...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TOKYO (Reuters) - Japan s Coast Guard found th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our good friend Brian Pannebecker is a tireles...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERLIN (Reuters) - The German military, buoyed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now that Donald Trump is president and Jeff Se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Perhaps the Green Party isn t so useless after...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The left is truly becoming unhinged! The tensi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If you ve been watching the Republican Nationa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. House of Repre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_fake\n",
       "0  UNITED NATIONS (Reuters) - The United States a...        0\n",
       "1  SYDNEY (Reuters) - Thousands of people rallied...        0\n",
       "2  TOKYO (Reuters) - Japan s Coast Guard found th...        0\n",
       "3  Our good friend Brian Pannebecker is a tireles...        1\n",
       "4  BERLIN (Reuters) - The German military, buoyed...        0\n",
       "5  Now that Donald Trump is president and Jeff Se...        1\n",
       "6  Perhaps the Green Party isn t so useless after...        1\n",
       "7  The left is truly becoming unhinged! The tensi...        1\n",
       "8  If you ve been watching the Republican Nationa...        1\n",
       "9  WASHINGTON (Reuters) - The U.S. House of Repre...        0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_news.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filtering data\n",
    "Removing stopwords and punctuation from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets are faster than lists when checking for \"not in\"\n",
    "stopword_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "punctuation_set = set(string.punctuation)\n",
    "# exclude_set = stopword_set.union(punctuation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the words from text\n",
    "def remove_stopwords(text: str, stop_set):\n",
    "    word_list = text.split()\n",
    "    filtered = \"\"\n",
    "    for word in word_list:\n",
    "        if word not in stopword_set:\n",
    "            filtered += word\n",
    "        filtered += \" \"\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# removing punctuation from text\n",
    "def remove_punctuation(text: str, punc_set):\n",
    "    #     filtered = \"\"\n",
    "    #     for char in text:\n",
    "    #         if char not in punc_set:\n",
    "    #             filtered += char\n",
    "    return \"\".join(char if char not in punc_set else \"\" for char in text)\n",
    "\n",
    "\n",
    "def filter_text(text: str, stopwords, punctuation):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text, punctuation)\n",
    "    text = remove_stopwords(text, stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text filtering these to the text fields\n",
    "df_all_news[\"text\"] = df_all_news[\"text\"].apply(\n",
    "    filter_text, args=(stopword_set, punctuation_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split dataset into train, validation, and test dataset\n",
    "Here we use a 60/20/20 train/validation/test split\n",
    "(This solution is a bit hacky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a 75:25 train/test split\n",
    "random_num = \"96\"\n",
    "random_num = int(random_num[::-1])\n",
    "\n",
    "# train = 0.8, test = 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_all_news[\"text\"],\n",
    "    df_all_news[\"is_fake\"],\n",
    "    test_size=0.2,\n",
    "    random_state=random_num,\n",
    ")\n",
    "\n",
    "# val = 0.25 * 0.8 = 0.2,\n",
    "# train = 0.75 * 0.8 = 0.6\n",
    "# test = 0.2 (unchanged)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=random_num,\n",
    ")\n",
    "\n",
    "# aliasing for our variables\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "val_labels = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14134\n",
       "0    12804\n",
       "Name: is_fake, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4706\n",
       "0    4274\n",
       "Name: is_fake, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above 2 value counts, we can see that the data is pretty well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing parameters\n",
    "vocab_size = 10000\n",
    "max_len = 300\n",
    "padding_type = \"post\"\n",
    "trunc_type = \"post\"\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# tokenize train and test data\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# pad our sequences\n",
    "train_seq_padded = sequence.pad_sequences(\n",
    "    train_seq, maxlen=max_len, padding=padding_type, truncating=trunc_type\n",
    ")\n",
    "test_seq_padded = sequence.pad_sequences(\n",
    "    test_seq, maxlen=max_len, padding=padding_type, truncating=trunc_type\n",
    ")\n",
    "val_seq_padded = sequence.pad_sequences(\n",
    "    val_seq, maxlen=max_len, padding=padding_type, truncating=trunc_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use GloVe embedding\n",
    "Download the embedding file [here](http://nlp.stanford.edu/data/glove.twitter.27B.zip) and place it into `../data/kaggle`\n",
    "Alternatively, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘../data/kaggle/glove.twitter.27B.zip’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber http://nlp.stanford.edu/data/glove.twitter.27B.zip -P ../data/kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative location of our embedding file\n",
    "EMBEDDING_FILE = \"../data/kaggle/glove.twitter.27B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")\n",
    "\n",
    "\n",
    "embeddings_index = dict(\n",
    "    get_coefs(*o.rstrip().rsplit(\" \")) for o in open(EMBEDDING_FILE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/miniconda3/envs/nlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index))\n",
    "\n",
    "# create embedding matrix\n",
    "# change below line if computing normal stats is too slow\n",
    "embedding_matrix = embedding_matrix = np.random.normal(\n",
    "    emb_mean, emb_std, (nb_words, embed_size)\n",
    ")\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define some training parameters and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override previously set embed_size when reading GloVe embedding\n",
    "embed_size = 100\n",
    "num_epochs = 5  # even this low number of epochs is overkill, see val_accuracy below\n",
    "batch = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Model 1: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300, 128)          84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,129,921\n",
      "Trainable params: 1,129,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define our LSTM model\n",
    "model_LSTM = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model_LSTM.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26938 samples, validate on 8980 samples\n",
      "Epoch 1/5\n",
      "26938/26938 [==============================] - 124s 5ms/sample - loss: 0.0874 - accuracy: 0.9583 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
      "Epoch 2/5\n",
      "26938/26938 [==============================] - 121s 4ms/sample - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9989\n",
      "Epoch 3/5\n",
      "26938/26938 [==============================] - 113s 4ms/sample - loss: 9.6219e-04 - accuracy: 0.9997 - val_loss: 0.0068 - val_accuracy: 0.9987\n",
      "Epoch 4/5\n",
      "26938/26938 [==============================] - 113s 4ms/sample - loss: 7.5588e-04 - accuracy: 0.9999 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 5/5\n",
      "26938/26938 [==============================] - 113s 4ms/sample - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0042 - val_accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5b7eb7e0b8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.fit(\n",
    "    train_seq_padded,\n",
    "    train_labels,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch,\n",
    "    validation_data=(val_seq_padded, val_labels),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 128)          64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,072,449\n",
      "Trainable params: 1,072,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model_CNN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26938 samples, validate on 8980 samples\n",
      "Epoch 1/5\n",
      "26938/26938 [==============================] - 21s 788us/sample - loss: 0.1327 - accuracy: 0.9518 - val_loss: 0.0065 - val_accuracy: 0.9989\n",
      "Epoch 2/5\n",
      "26938/26938 [==============================] - 21s 773us/sample - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "Epoch 3/5\n",
      "26938/26938 [==============================] - 21s 772us/sample - loss: 5.7045e-04 - accuracy: 0.9999 - val_loss: 0.0032 - val_accuracy: 0.9994\n",
      "Epoch 4/5\n",
      "26938/26938 [==============================] - 21s 777us/sample - loss: 3.9587e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "Epoch 5/5\n",
      "26938/26938 [==============================] - 21s 776us/sample - loss: 3.8474e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5abc5cd278>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CNN.fit(\n",
    "    train_seq_padded,\n",
    "    train_labels,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch,\n",
    "    validation_data=(val_seq_padded, val_labels),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Model 3: Simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300, 256)          25856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,864,705\n",
      "Trainable params: 10,864,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define our LSTM model\n",
    "model_DNN = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model_DNN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_DNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26938 samples, validate on 8980 samples\n",
      "Epoch 1/5\n",
      "26938/26938 [==============================] - 30s 1ms/sample - loss: 0.1087 - accuracy: 0.9457 - val_loss: 0.0040 - val_accuracy: 0.9991\n",
      "Epoch 2/5\n",
      "26938/26938 [==============================] - 29s 1ms/sample - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9986\n",
      "Epoch 3/5\n",
      "26938/26938 [==============================] - 29s 1ms/sample - loss: 5.7403e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9992\n",
      "Epoch 4/5\n",
      "26938/26938 [==============================] - 29s 1ms/sample - loss: 3.7113e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9992\n",
      "Epoch 5/5\n",
      "26938/26938 [==============================] - 30s 1ms/sample - loss: 4.3211e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5abc41cc50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DNN.fit(\n",
    "    train_seq_padded,\n",
    "    train_labels,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch,\n",
    "    validation_data=(val_seq_padded, val_labels),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Calculating F1 scores and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 1, Bidirectional LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26938/26938 [==============================] - 50s 2ms/sample - loss: 6.5511e-04 - accuracy: 0.9999\n",
      "8980/8980 [==============================] - 17s 2ms/sample - loss: 0.0088 - accuracy: 0.9980\n",
      "Bidirectional LSTM Accuracy: Train = 0.9999257326126099 Test = 0.9979955554008484\n",
      "Bidirectional LSTM F1-Score = 0.9980879541108987\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, train_acc_lstm = model_LSTM.evaluate(train_seq_padded, train_labels)\n",
    "_, test_acc_lstm = model_LSTM.evaluate(test_seq_padded, test_labels)\n",
    "print(f\"Bidirectional LSTM Accuracy: Train = {train_acc_lstm} Test = {test_acc_lstm}\")\n",
    "# f1 score\n",
    "label_pred_lstm = model_LSTM.predict_classes(test_seq_padded)\n",
    "label_pred_lstm = label_pred_lstm[:]\n",
    "f1_lstm = f1_score(test_labels, label_pred_lstm)\n",
    "print(f\"Bidirectional LSTM F1-Score = {f1_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 2, CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26938/26938 [==============================] - 4s 167us/sample - loss: 3.2971e-04 - accuracy: 1.0000\n",
      "8980/8980 [==============================] - 1s 153us/sample - loss: 0.0051 - accuracy: 0.9982\n",
      "CNN Accuracy: Train = 0.9999628663063049 Test = 0.9982182383537292\n",
      "CNN F1-Score = 0.9982996811902233\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, train_acc_cnn = model_CNN.evaluate(train_seq_padded, train_labels)\n",
    "_, test_acc_cnn = model_CNN.evaluate(test_seq_padded, test_labels)\n",
    "print(f\"CNN Accuracy: Train = {train_acc_cnn} Test = {test_acc_cnn}\")\n",
    "# f1 score\n",
    "label_pred_cnn = model_CNN.predict_classes(test_seq_padded)\n",
    "label_pred_cnn = label_pred_cnn[:]\n",
    "f1_cnn = f1_score(test_labels, label_pred_cnn)\n",
    "print(f\"CNN F1-Score = {f1_cnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 3: Simple DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26938/26938 [==============================] - 11s 417us/sample - loss: 3.2804e-04 - accuracy: 1.0000\n",
      "8980/8980 [==============================] - 4s 427us/sample - loss: 0.0031 - accuracy: 0.9991\n",
      "DNN Accuracy: Train = 0.9999628663063049 Test = 0.999109148979187\n",
      "DNN F1-Score = 0.9991500212494687\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, train_acc_dnn = model_DNN.evaluate(train_seq_padded, train_labels)\n",
    "_, test_acc_dnn = model_DNN.evaluate(test_seq_padded, test_labels)\n",
    "print(f\"DNN Accuracy: Train = {train_acc_dnn} Test = {test_acc_dnn}\")\n",
    "# f1 score\n",
    "label_pred_dnn = model_DNN.predict_classes(test_seq_padded)\n",
    "label_pred_dnn = label_pred_dnn[:]\n",
    "f1_dnn = f1_score(test_labels, label_pred_dnn)\n",
    "print(f\"DNN F1-Score = {f1_dnn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view our training results\n",
    "# this launches TensorBoard in this cell\n",
    "# %tensorboard --logdir $log_dir # oops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Summary\n",
    "## Model Choice\n",
    "Model 1 -> LSTM: [Source](https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S)\n",
    "Derived from Moroney's NLP Zero to Hero series on YouTube, I chose to train this because I wanted to see how a simple model that came from a mere ~4 minute tutorial, also because LSTMs are also good for sequences of data which makes it a good fit for this ML task. Furthermore, the first LSTM layer is set to be bi-directional.\n",
    "\n",
    "Model 2 -> CNN: I chose to train this because coming from a computer vision background, where CNNs are mostly used, I wanted to see how this model architecture would do against the LSTM model. I also chose this model as CNNs are able to learn the features, stitching lower level features to higher level features.\n",
    "\n",
    "Model 3 -> Simple DNN: I chose to train this to see how a simple DNN with few layers and a low number of neurons would go against the more powerful Bidirectional LSTM and CNN models.\n",
    "\n",
    "## Model Performance\n",
    "| Model | Accuracy (Test) | F1-Score |\n",
    "| --- | --- | --- |\n",
    "| Bidirectional LSTM | 0.9979955554008484 | 0.9980879541108987 |\n",
    "| CNN | 0.9982182383537292 | 0.9982996811902233 |\n",
    "| DNN | 0.999109148979187 | 0.9991500212494687 |\n",
    "\n",
    "Using a train/validation/test split of 60/20/20, the following accuracy on the test dataset and F1-Scores were achieved.\n",
    "\n",
    "In terms of F1-Score, DNN > CNN > Bidirectional LSTM. However, the difference in accuracy is negligible, (there is a maximum difference of 0.11%).\n",
    "\n",
    "In terms of accuracy of the 3 models, DNN > CNN > Bidirectional LSTM. However, the difference in accuracy is negligible, (there is a maximum difference of 0.12%).\n",
    "\n",
    "In terms of training speed, CNN > DNN > Bidirectional LSTM. \n",
    "\n",
    "The most preferred model is the CNN, which has a quick training speed, as well as decent performance. However, we have to consider that the dataset is very small, where the combined file size of`True.csv` and `Fake.csv` is only 111MB. If the dataset were larger, I would choose the Bidirectional LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
